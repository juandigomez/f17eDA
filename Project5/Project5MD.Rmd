---
title: "Project 5: Breast Cancer"
author: "Group 8: Jackie Lee, Juan Gomez, Patrick Lyons, Ross Miglin"
resource_files:
- .Renviron
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 6
    toc_float: yes
runtime: shiny
---

```{r setup, include=FALSE}
require(data.world)
require(MASS)
require(ISLR)
require(dplyr)
require(ggplot2)
require(hexbin)
require(jsonlite)
require(shiny)
require(rsconnect)
require(knitr)
require(lubridate)
require(rmarkdown)
require(tidyr)
require(glmnet)
library(MASS)
library(ISLR)
library(ggplot2)
library(class)
library(glmnet)
library(leaps)
library("e1071")
library(pROC)
library(ROCR)
require(ROCR)
require(pROC)
require(tree)
require(randomForest)
require(gbm)
require(boot)
knitr::opts_chunk$set(echo = TRUE)
```

## **R Session Info**
```{r}
sessionInfo()
```

## **Introduction**
For this project, we will use all methods and algorthims that we have been investigated this past semester. From Linear regression to Linear Discriminant Analysis to Decision Trees and everything in between. With this array of methods in our tool box, we can analyze the data and find some intresting insights based on the kinds of methods we use.


## **Requiring data and initializing session**
We have retrieved our data and required the proper libraries in order to correctly compute our analysis. Our data can be viewed at https://data.world/patrickyoutube/f-17-eda-project-5.
```{r}
require(data.world)

project <- "https://data.world/patrickyoutube/f-17-eda-project-5"
data.world::set_config(cfg_env("DW_API"))
df <- data.world::query(
  data.world::qry_sql("SELECT * FROM data"),
  dataset = project
)

attach(df)


```

## **Interesting Findings**

**Using Best Subset Regression results in LDA**
```{r}
train1 <- sample(1:568, 284)
diagnosis2=lda(diagnosis~radius_mean+compactness_mean+concavity_mean+concave_points_mean+radius_se+smoothness_se+concavity_se+concave_points_se+radius_worst+texture_worst+area_worst+concavity_worst+symmetry_worst+fractal_dimension_worst,data=df,prior=c(1,1)/2,subset=train1)
lda.pred2=predict(diagnosis2,df[-train1, ])
df2 = data.frame(lda.pred2)
table(lda.pred2$class,df[-train1, ]$diagnosis)
mean(lda.pred2$class==df[-train1, ]$diagnosis)
```

We decided to use the results from Best Subset Regression in LDA to see what kind of result we would get. Surprisingly, the misclassification rate is even lower than the misclassification rate from the original LDA model. This is interesting because we were able to utilize the findings from best subset regression to find an even better LDA model.

## **Multi-predictor Linear Regression**
```{r}
fit=lm(radius_mean~concave_points_mean+concavity_mean,data=df)
summary(fit)
par(mfrow=c(2,2))
renderPlot(plot(fit))
```

Because our response variable is categorical (whether the tumor is malignant or benign), we decided to make radius_mean as the response variable for linear regression because it's a quantitative variable. We think using radius_mean as the response variable is fine in this case because malignant tumors typically grow much quicker compared to benign tumors. After plotting several scatterplots, concavity_mean and concave_points_mean appear to be the best predictors for radius_mean.

Looking at the residual plot, the points are randomly scattered, so we can assume that the model is approximately linear with a constant variance. Additionally, the Normal Q-Q plot is close to being linear which means that the model is approximately normal. We have a R ^2 value of 0.7201, which is fairly decent. We can conclude that concavity_mean and concave_points_mean are good predictors for radius_mean which in turn should hopefully mean that they are good predictors for diagnosis.

## **Residual Analysis**

## **Logistic Regression**

## **Linear Discriminant Analysis**
```{r}
diagnosis1=lda(diagnosis~radius_mean+perimeter_mean+area_mean+compactness_mean+concavity_mean,data=df,prior=c(1,1)/2,subset=train1)
lda.pred=predict(diagnosis1, df[-train1, ])
df1 = data.frame(lda.pred)
sliderInput("bins", "Number of bins:", min = 10, max = 50, value = 30)

renderPlot({
  x <- df1$LD1
  bins <- seq(min(x), max(x), length.out = input$bins + 1)
  ggplot(df1) + geom_histogram(mapping = aes(x=LD1), breaks = bins, color="blue") + facet_wrap(~class)})
renderPlot(ggplot(df1) + geom_boxplot(mapping = aes(x=class, y=LD1)))

table(lda.pred$class,df[-train1, ]$diagnosis)
mean(lda.pred$class==df[-train1, ]$diagnosis)
```
After running each of the "mean" variables separately in LDA, we picked the best variables to run the analysis. The misclassification rate is low and there's no overlap in LD1. 

## **Quadratic Discriminant Analysis**

```{r}
qda.fit = qda(diagnosis~texture_mean+perimeter_mean+area_mean+radius_mean,data=df,family=binomial)
qda.fit
df.700 = subset(df,area_mean>700)
qda.class = predict(qda.fit, df.700)
table(qda.class$class,df.700$diagnosis)
mean(qda.class$class==df.700$diagnosis)
```

Upon running QDA on several mean measurements of tumors, we found that the best predictor was the mean area of a tumor. In the interval of tumors greater than 700 square units, we predict tumors correctly 95.32% of times. This will be a good predictor for our later insights.  

## **K-Nearest Neighbors**

With K-Nearest Neighbors analysis or KNN, we can use multiple predictors to see which one is best at predicting the results of having a cancerous tumor or not. We set and training data based on the id numbers to use in our analysis. The first KNN analysis done we used perimeter_mean and area_mean and got a results of mean of 83% which is incredibly accurate.
```{r}
# Establishing training data
train = id > 900000

# Test 3: Using resting blood preassure and chest pains to determine heart disease
test = cbind(df$perimeter_mean,df$area_mean)

knn.pred = knn(test[train,], test[!train,], df$diagnosis[train], k=1)
table(knn.pred,df$diagnosis[!train])
mean(knn.pred==df$diagnosis[!train])
```

Since these two parameters gave great results, we can use variations of them to see if we can get better results. For this analysis we used perimeter_mean again but used area_se instead. We got much better results with a mean of 84%. 
```{r}
# Establishing training data
train = id > 900000

# Test 3: Using resting blood preassure and chest pains to determine heart disease
test = cbind(df$perimeter_mean,df$area_se)

knn.pred = knn(test[train,], test[!train,], df$diagnosis[train], k=1)
table(knn.pred,df$diagnosis[!train])
mean(knn.pred==df$diagnosis[!train])
```

Now that we have improved our results, we can see if we can continue to use the parameters but change our K values, we could potentially get even better results. The analysis consistently determines that a k value of 3 will give the highest mean which is 86%.
```{r}
# Establishing training data
train = id > 900000

# Test 3: Using resting blood preassure and chest pains to determine heart disease
test = cbind(df$perimeter_mean,df$area_se)

mean = 1:20
max = 0

for(k in 1:20){
  knn.pred = knn(test[train,], test[!train,], df$diagnosis[train], k)
  mean[k] = mean(knn.pred==df$diagnosis[!train])
  if(mean[k] > max){
     max = mean[k]
     maxk = k
  }
}

renderPlot(plot(mean))

max
maxk

```
## **ROC Curve**
The ROC curve stands for Receiver Operating Characteristic. It provides us with a visual of our true positive rate against our false positive rate as a diagnostic for interpreting our predictors' accuracy. When the curve is a steeper slope, we have higher sensitivity (which is our goal). When the curve is less steep, we have a less accurate test

Below is the ROC curve for smoothness mean:
```{r}

roc(diagnosis ~ smoothness_mean,df)
roc(diagnosis ~ smoothness_mean, df, smooth=TRUE)
renderPlot(plot.roc(df$diagnosis, df$smoothness_mean, percent = TRUE,
            partial.auc=c(100,90), partial.auc.correct= TRUE,
            partial.auc.focus="sens",
            ci=TRUE, boot.n=100, ci.alpha=0.9, stratified=FALSE,
            auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE))
roc(df$diagnosis, df$smoothness_mean)
```
We see that the slope is rather flat (close to 45 degrees). Our test accuracy using smoothness_mean will be only fair. With an area under the curve of 0.7235 (perfect is 1 and failing is 0.5), our predictor is just average.
```{r}
renderPlot(plot.roc(df$diagnosis, df$compactness_mean, percent = TRUE,
            partial.auc=c(100,90), partial.auc.correct= TRUE,
            partial.auc.focus="sens",
            ci=TRUE, boot.n=100, ci.alpha=0.9, stratified=FALSE,
            auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE))
roc(df$diagnosis,df$compactness_mean)
```
Visually, we can see that compactness_mean has a steeper slope. Our value for area under the curve is 0.8638 proving compactness_mean to be a better predictor than smoothness_mean.
```{r}
renderPlot(plot.roc(df$diagnosis, df$area_mean, percent = TRUE,
            partial.auc=c(100,90), partial.auc.correct= TRUE,
            partial.auc.focus="sens",
            ci=TRUE, boot.n=100, ci.alpha=0.9, stratified=FALSE,
            auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE))
roc(df$diagnosis,df$area_mean)
```
Since QDA showed area_mean to be a solid predictor, I hypothesized that the slope of area_mean would be steeper than that of smoothness and compactness. Just as I suspected, the area under the curve for area_mean came out to 0.9383, proving the area_mean predictor to have very high selectivity.

## **Validation**
Validation consists of breaking our data up into separate segments. Using Leave-one-out cross validation, we found the following plot:
```{r}
df$diagnosis[df$diagnosis == "M"] <- 1
df$diagnosis[df$diagnosis == "B"] <- 0
df$diagnosis <- as.numeric(df$diagnosis)

renderPlot(plot(df$diagnosis~df$texture_mean,data=df))

## LOOCV
glm.fit=glm(df$diagnosis~df$texture_mean, data=df)

loocv=function(fit){
  h=lm.influence(fit)$h
  mean((residuals(fit)/(1-h))^2)
}

loocv(glm.fit)
```

Using LOOCV, I was able to parse through the texture_mean data and see its correlation in comparison with the diagnosis of malignant vs. benign. LOOCV takes a single observation from the validation data and uses the other observations in the training data. As the model is created, each observation within our sample is used in the validation data.

## **Cross-Validation**
Choosing a sample training set for which to train upon requires that our sample be representative of the data as a whole. As a result, the validation process of randomly splitting the data in half can yield highly variable test error. We are going to perform K-fold Cross validation for K=5 and K=10 to estimate our test error and then see how they stack up both to each other and to LOOCV which I ran previously.
```{r}
df$diagnosis[df$diagnosis == "M"] <- 1
df$diagnosis[df$diagnosis == "B"] <- 0
df$diagnosis <- as.numeric(df$diagnosis)
## 5-fold CV error
cv.error=rep(0,5)
degree=1:5
for(d in degree){
  glm.fit=glm(diagnosis~poly(texture_mean,d), data=df)
  cv.error[d]=loocv(glm.fit)
}


## 10-fold CV error
cv.error10=rep(0,5)
for(d in degree){
  glm.fit=glm(diagnosis~poly(texture_mean,d), data=df)
  cv.error10[d]=cv.glm(df,glm.fit,K=10)$delta[1]
}
renderPlot(plot(degree,cv.error,type="b")+lines(degree,cv.error10,type="b",col="red"))
#renderPlot(plot(degree,cv.error10,type="b",col="red"))
```
The black line represents our 5-fold cross-validation process and the red line represents our 10-fold cross-validation. Looking at the same 5 subdivisions for each cross-validation, we see a reduction in error simply from increasing the amount of subdivisions, K, by 5.

One short-coming of using K-fold Cross-validation is that the prediction error will be biased upward due to high bias-variance tradeoff. If we choose, LOOCV, we minimize bias but greatly increase variance since K=n. If we choose something like K=5 of 10, our tradeoff is a little better.

## **The Bootstrap**
We choose to create a bootstrap method in addition to cross-validation because bootstrapping allows us to quantify uncertainty for a certain statistical learning method. We have written a function for calculating alpha of the predictor and response variables (texture_mean and tumor diagnosis respectively).


```{r}
alpha=function(x,y){
  vx=var(x)
  vy=var(y)
  cxy=cov(x,y)
  (vy-cxy)/(vx+vy-2*cxy)
}
alpha(df$texture_mean,df$diagnosis)

## What is the standard error of alpha?

alpha.fn=function(data, index){
  with(data[index,],alpha(texture_mean,diagnosis))
}

alpha.fn(df,1:100)

set.seed(1)
alpha.fn (df,sample(1:100,100,replace=TRUE))

boot.out=boot(df,alpha.fn,R=1000)
boot.out
renderPlot(plot(boot.out))
```
Our histogram shows estimates of the alpha values we obtained by generating 1,000 simulated data sets based on the true population. Our mean centers around -0.03704653 which is our true value for alpha. Alpha is a indication of how well our risk has been minimized.

## **Subset Selection**

**Best Subset Selection**
Using best subset selesction we can detrmine which model has the best predictors with the lowest error. With a dataset as large and complex as the one we are using, this helps us narrow down some key predictors.
```{r}
df$diagnosis[df$diagnosis == "M"] <- 1
df$diagnosis[df$diagnosis == "B"] <- 0

#RUnning best subset regression and saving results
regfit.full=suppressWarnings(regsubsets(df$diagnosis~.,data=df, nv=32))
reg.summary=summary(regfit.full)

#Plotting results
min = which.min(reg.summary$cp)
renderPlot(plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp")
           + points(min,reg.summary$cp[min],pch=20,col="red"))
```

From the analysis above we can see that that the model with the lowest error has 14 different predictors. Those predictors can be seen below in the summary of the coefficients of the best subset analysis for this model.
```{r}
#Coefficient Summary
coef(regfit.full,min)
```

**Forward Stepwise**
Now we that we have a genral idea of what kind of results subset selection can reproduce, we can try the analysis again this time using forward stepwise. As one can see from the results below, this analysis yields a model with 16 parameters as the best.
```{r}
df$diagnosis[df$diagnosis == "M"] <- 1
df$diagnosis[df$diagnosis == "B"] <- 0

#RUnning best subset regression and saving results
regfit.forward=suppressWarnings(regsubsets(df$diagnosis~.,data=df, nv=32, method = "forward"))
reg.summary1=summary(regfit.forward)

#Plotting results
min1 = which.min(reg.summary1$cp)
renderPlot(plot(reg.summary1$cp,xlab="Number of Variables",ylab="Cp")
           + points(min1,reg.summary1$cp[min1],pch=20,col="red"))
```

Some of the parameters are also found in the results from the best subset selection but we also have new parameters taken into account such as area_se and compactness_se.
```{r}
#Coefficient Summary
coef(regfit.forward,min)
```

**Backward Stepwise**
Lastly we can repeat the analysis using backward stepwise. Interestingly enough, we actually get the same results as the original analysis, indluding the parameters that make up the best model as seen below.
```{r}
df$diagnosis[df$diagnosis == "M"] <- 1
df$diagnosis[df$diagnosis == "B"] <- 0

#RUnning best subset regression and saving results
regfit.backward=suppressWarnings(regsubsets(df$diagnosis~.,data=df, nv=32, method = "backward"))
reg.summary2=summary(regfit.backward)

#Plotting results
min2 = which.min(reg.summary2$cp)
renderPlot(plot(reg.summary2$cp,xlab="Number of Variables",ylab="Cp")
           + points(min,reg.summary2$cp[min2],pch=20,col="red"))

#Coefficient Summary
coef(regfit.backward,min)
```

## **Shrinkage Methods**
**Ridge Regression**
Continuing our analysis, we can use shrinkage methods to see which model would give us the best results as well. First we must make a matrix our data and a variable y with the response variable.
```{r}
x=model.matrix(df$diagnosis~.-1,data=df) 
y=df$diagnosis
y = as.numeric(y)
```

Now we can use that matrix and the response variable to run a ridge regression model using glmnet. THis allows us to visually see the coefficients of the parameters that most affect the model and then it shrinks those coefficients to almost zero.
```{r}
fit.ridge=glmnet(x,y,alpha=0)
renderPlot(plot(fit.ridge,xvar="lambda",label=TRUE))
```

We are able to cross validate this model to see which one produces best results and according to the curve, the full model tends to be showing pretty good results. When looking at the coefficients, we can see that all parameters are in use except for id.
```{r}
cv.ridge=cv.glmnet(x,y,alpha=0)
renderPlot(plot(cv.ridge))
coef(cv.ridge)
```

**Lasso**
SImilarly, we can use glmnet to run an analysis using the lasso model. This graphs looks similar slightly different than the ridge regression model. It seems like lasso is determine that other coefficients have more weight than others.
```{r}
fit.lasso=glmnet(x,y)
renderPlot(plot(fit.lasso,xvar="lambda",label=TRUE))
```

When running cross validation on the lasso model, we can see that while many parameters are still in use, some were not signifacnt enough and were not included in the model.
```{r}
cv.lasso=cv.glmnet(x,y)
renderPlot(plot(cv.lasso))
coef(cv.lasso)
```



## **Decision Trees**
Decision trees are great for this kind of analysis because he already have a binary response variable. We have to do some set up first in order for the tree to analyz the data correctly.
```{r}
# Setting up the data
df$diagnosis[df$diagnosis == 1] <- "M"
df$diagnosis[df$diagnosis == 0] <- "B"

result = df$diagnosis

df$diagnosis[df$diagnosis == "M"] <- 1
df$diagnosis[df$diagnosis == "B"] <- 0

df$diagnosis = as.numeric(df$diagnosis)

df=data.frame(df, result)
```

Once this set up is compelete, we can do a tree and analyze the kinds of parameters and thresholds it deems significant.
```{r}
tree.diagnosis=tree(result~.-diagnosis,df)
summary(tree.diagnosis)
renderPlot({
  plot(tree.diagnosis)
  text(tree.diagnosis,pretty=0, cex = 0.7)
})
```

After producing the tree above we can see that the big factor seems to be perimeter_worst. Most of the predictions on the right or greater than 105.95 seem to produce a result showing a malignant tumor. Meanwhile, predictions using perimeter_worst less than 1005.95 mostly seem to produce reults of benign tumors.

**Random Forest**
We continue our analysis of using decision trees this time as random forest. Random forrest reproduces many bushy trees and averages the error. The biggest variable we can change with this is the size of the training data.
```{r}
set.seed(101)
train=sample(1:nrow(df),100)

rf.diagnosis=randomForest(result~.-diagnosis,data=df,subset=train)
rf.diagnosis
```

With a training data of 100, we got incredibly accurate results with an error rate of 6%. 
```{r}
set.seed(101)
train=sample(1:nrow(df),200)

rf.diagnosis=randomForest(result~.-diagnosis,data=df,subset=train)
rf.diagnosis
```

Increasing the size of the training data improved the accuracy slightly by repoducing an error rate of 5%
```{r}
set.seed(101)
train=sample(1:nrow(df),400)

rf.diagnosis=randomForest(result~.-diagnosis,data=df,subset=train)
rf.diagnosis
```

Finally, creating a training data of size 400, we get the best results with a 3% error rate, which is increadibly accurate results.This again can be due to the nature of the dataset which seems to be set up very well for this kind of analysis.

**Boosting**
Boosting is another way to analyze decision trees. This method instead creates many small tree. The function allows us to play arround with the amount of trees, the depth and the division.

```{r}
set.seed(101)
train=sample(1:nrow(df),400)

boost.diagnosis=gbm(result~.-diagnosis,data=df[train,],distribution="gaussian",n.trees=1000,shrinkage=0.001,interaction.depth=2)
renderPlot(summary(boost.diagnosis))
```

```{r}
set.seed(101)
train=sample(1:nrow(df),400)

boost.diagnosis=gbm(result~.-diagnosis,data=df[train,],distribution="gaussian",n.trees=1000,shrinkage=1,interaction.depth=3)
renderPlot(summary(boost.diagnosis))
```

```{r}
set.seed(101)
train=sample(1:nrow(df),400)

boost.diagnosis=gbm(result~.-diagnosis,data=df[train,],distribution="gaussian",n.trees=1000,shrinkage=0.1,interaction.depth=4)
renderPlot(summary(boost.diagnosis))
```

We ran three analysis, each with a different shrinkage and depth value but the results remained consistent. The most significant predictors using boosting are concave_points_worst, area_worst, and premieter_worst.

## **Support Vector Machines**

## **Unsupervised Learning**

**PCA**
```{r}
breastmatrix = matrix(
  c(radius_mean,texture_mean,perimeter_mean,area_mean,smoothness_mean,compactness_mean,concavity_mean,concave_points_mean,symmetry_mean,fractal_dimension_mean,radius_se,texture_se,perimeter_se,area_se,smoothness_se,compactness_se,concavity_se,concave_points_se,symmetry_se,fractal_dimension_se,radius_worst,texture_worst,perimeter_worst,area_worst,smoothness_worst,compactness_worst,concavity_worst,concave_points_worst,symmetry_worst,fractal_dimension_worst), 569, 30)

dimnames(breastmatrix) = list(
  c("M","M","M","M","B","B","B","B","M", "B", "B", "M", "B", "M", "B", "B", "B", "B", "B", "B", "B", "M", "M", "M", "M", "B","M", "M", "B", "B", "B", "M", "M", "B", "M", "M", "B", "M", "M", "B", "M", "M", "B", "B", "M", "B", "M", "B", "M", "B","B", "B","B", "M", "B", "B", "B", "M", "B", "B", "B", "B", "B", "B", "M", "B", "B", "B", "M", "B", "M", "M", "B", "M", "B", "B", "M", "M","B", "M", "B", "M", "M", "B", "B", "B", "B", "M", "B", "B", "M", "M", "M", "M", "B", "M", "B", "M", "B", "B", "B", "B", "M", "B", "B", "M", "M", "B", "M", "M", "M", "M", "M", "B", "B", "M","M", "M", "B", "M", "B", "M", "B", "B", "B", "M", "B", "M", "M", "M","M", "B", "B", "M", "M", "M", "B", "B", "B", "M", "B", "B", "B", "B", "B", "M", "M", "M", "B", "B", "M", "B", "B", "M", "M", "B","M", "M", "B", "B", "B", "B", "M", "B", "B", "B", "B", "B", "M", "M", "B", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M","M", "M", "M", "M", "B", "B", "B", "B", "M", "B", "B", "M", "B", "M", "B", "B", "M", "B", "B", "M", "M", "B", "M", "M", "B", "B","B","B", "B", "B", "M", "B", "B", "B", "B", "B", "B", "B", "M", "B", "B", "M", "M", "M", "B", "M", "B", "B", "B", "B", "B", "B","B", "M", "B", "B", "B", "B", "B", "B", "B", "M", "B", "B", "M", "B", "M", "B", "M", "B", "B", "B", "B", "M", "M", "M", "M", "B", "B", "B", "B", "M", "B", "M", "B", "M", "M", "B", "B", "B", "M", "B", "B", "B", "B", "B", "B", "M", "B", "M", "M", "M", "B", "B","B", "B", "B", "B", "M", "B", "B", "B", "B", "B", "M", "M", "B", "M", "M", "B", "M", "B", "M", "M", "B", "B", "B", "B", "B", "M","M", "B", "B", "B", "B", "B", "M", "B", "B", "B", "M", "M", "B", "B", "M", "M", "B", "B", "B", "B", "B", "B", "M", "M", "M", "B","B", "B", "B", "B", "B", "B", "M", "B", "M", "B", "B", "B", "B", "M", "B", "B", "M", "B", "B", "M", "B", "B", "B", "B", "B", "B","B", "B", "B", "B", "M", "M", "B", "M", "M", "B", "M", "B", "B", "B", "B", "M", "B", "M", "B", "B", "M", "B", "M", "B", "B", "M","M", "B", "M", "B", "B", "B", "B", "B", "B", "B", "B", "B", "M", "M", "B", "B", "B", "B", "B", "B", "M", "B", "M", "B", "B", "B","B", "B", "B", "B", "B", "B", "M", "B", "B", "B", "B", "B", "B", "B", "B", "M", "B", "M", "B", "B", "B", "M", "B", "B", "B", "B", "B", "M", "M", "M", "B", "B", "M", "B", "M", "B", "B", "B", "B", "B", "M", "B", "B", "B", "M", "B", "M", "B", "M", "M", "B", "B","B", "B", "M", "B", "B", "B", "B", "B", "B", "B", "B", "M","B", "B", "B", "M", "B", "M", "M", "B", "B", "B", "M", "B", "B", "B","B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "M", "B", "B", "M", "M", "M", "M", "M","M", "B", "M", "B", "B", "M", "B", "B", "M", "B", "M", "M", "B", "B", "B", "B", "M", "M", "B", "M", "M", "B", "M", "B", "M", "M","B", "M", "B", "B", "M", "M", "B", "M", "M", "M", "B", "B", "M", "B", "M", "B", "B", "M", "M", "B","B", "B", "M"),
  c("radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean","compactness_mean","concavity_mean","concave_points_mean","symmetry_mean","fractal_dimension_mean","radius_se","texture_se","perimeter_se","area_se","smoothness_se","compactness_se","concavity_se","concave_points_se","symmetry_se","fractal_dimension_se","radius_worst","texture_worst","perimeter_worst","area_worst","smoothness_worst","compactness_worst","concavity_worst","concave_points_worst","symmetry_worst","fractal_dimension_worst")
)

apply(breastmatrix,2,mean)
apply(breastmatrix,2, var)
pca.out=prcomp(breastmatrix, scale=TRUE)
renderPlot(biplot(pca.out, scale=0))
```

**K-Means Clustering**
```{r}
breastdf <- data.world::query(
  data.world::qry_sql("SELECT * FROM data"),
  dataset = project
)
renderPlot({
  ggplot(breastdf,(aes(x=radius_worst, y=concavity_worst, colour = as.factor(diagnosis)))) + geom_point()
})

set.seed(570)
km.out=kmeans(breastmatrix,2,nstart=15)
breastdf3 <- data.frame(breastdf, km.out$cluster)
breastdf4 <- data.frame(km.out$centers, km.out$size)
renderPlot({ggplot(breastdf3) + geom_point(mapping = aes(x=radius_worst, y=concavity_worst, colour = as.factor(km.out.cluster))) + geom_point(data=breastdf4, mapping=aes(radius_worst, concavity_worst, size=km.out.size))})
```

While there is some overlap, radius_worst and concavity_worst gives us clusters that are distinct which is what we expected because these variables were part of the best subset regression. Additionally, the "worst" variables most likely play into the distinction of malignant and benign observations because the values are extreme. 

**Hierachical Clustering**
```{r}
hc.complete=hclust(dist(breastmatrix),method="complete")
renderPlot(plot(hc.complete))
hc.cut=cutree(hc.complete,2)
table(hc.cut,diagnosis)
table(hc.cut,km.out$cluster)
```

The hierarchical dendrogram wasn't helpful because there are so many rows. But looking at the tables above, 81 of the malignant observations are included in cluster 2 which is dominated by benign observations. There seems to be a bigger overlap than we originally thought. 

## **Conclusion**
