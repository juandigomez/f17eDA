---
title: "Project 5: Breast Cancer"
author: "Group 8: Jackie Lee, Juan Gomez, Patrick Lyons, Ross Miglin"
resource_files:
- .Renviron
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 6
    toc_float: yes
runtime: shiny
---

```{r setup, include=FALSE}
require(data.world)
require(MASS)
require(ISLR)
require(dplyr)
require(ggplot2)
require(hexbin)
require(jsonlite)
require(shiny)
require(rsconnect)
require(knitr)
require(lubridate)
require(rmarkdown)
require(tidyr)
require(glmnet)
library(MASS)
library(ISLR)
library(ggplot2)
library(class)
library(glmnet)
library(leaps)
library("e1071")
library(pROC)
library(ROCR)
require(ROCR)
require(pROC)
require(tree)
require(randomForest)
require(gbm)
require(boot)
knitr::opts_chunk$set(echo = TRUE)
```

## **R Session Info**
```{r}
sessionInfo()
```

## **Introduction**
For this project, we will use all methods and algorthims that we have been investigated this past semester. From Linear regression to Linear Discriminant Analysis to Decision Trees and everything in between. With this array of methods in our tool box, we can analyze the data and find some intresting insights based on the kinds of methods we use.


## **Requiring data and initializing session**
We have retrieved our data and required the proper libraries in order to correctly compute our analysis. Our data can be viewed at https://data.world/patrickyoutube/f-17-eda-project-5.
```{r}
require(data.world)

project <- "https://data.world/patrickyoutube/f-17-eda-project-5"
data.world::set_config(cfg_env("DW_API"))
df <- data.world::query(
  data.world::qry_sql("SELECT * FROM data"),
  dataset = project
)

breastdf1 <- data.world::query(
  data.world::qry_sql("SELECT * FROM data"),
  dataset = project
)
attach(df)
bssdf=df
smdf=df

```

## **Interesting Findings**

**Data.world Insights**
The following are some of the insights we developed during this project. The rest can be found in our data.world project found here: https://data.world/patrickyoutube/f-17-eda-project-5/insights

**Using Best Subset Regression results in LDA**
```{r}
train1 <- sample(1:568, 284)
diagnosis2=lda(diagnosis~radius_mean+compactness_mean+concavity_mean+concave_points_mean+radius_se+smoothness_se+concavity_se+concave_points_se+radius_worst+texture_worst+area_worst+concavity_worst+symmetry_worst+fractal_dimension_worst,data=breastdf1,prior=c(1,1)/2,subset=train1)
lda.pred2=predict(diagnosis2,breastdf1[-train1, ])
df2 = data.frame(lda.pred2)
table(lda.pred2$class,breastdf1[-train1, ]$diagnosis)
mean(lda.pred2$class==breastdf1[-train1, ]$diagnosis)
```
We decided to use the results from Best Subset Regression in LDA to see what kind of result we would get. Surprisingly, the misclassification rate is even lower than the misclassification rate from the original LDA model. This is interesting because we were able to utilize the findings from best subset regression to find an even better LDA model.

**Using Best Subset Regression results in KNN**
```{r}
# Establishing training data
train = id > 900000

# Test 3: Using resting blood preassure and chest pains to determine heart disease
test = cbind(radius_mean,compactness_mean,concavity_mean,concave_points_mean,radius_se,smoothness_se,concavity_se,concave_points_se,radius_worst,texture_worst,area_worst,concavity_worst,symmetry_worst,fractal_dimension_worst)

knn.pred = knn(test[train,], test[!train,], df$diagnosis[train], k=1)
table(knn.pred,df$diagnosis[!train])
mean(knn.pred==df$diagnosis[!train])
```
We decided to use the results from Best Subset Regression in KNN as well to see what kind of result we would get. Unlike the LDA analysis, the mean for KNN was much larger than the original KNN analysis. This means that KNN is more sensitive to the parameters found by best subset regression.

**Using Best Logistic Regression Results**
By far the most interesting finding from my analysis was the ability of the logistic regression model to predict diagnosis using concave_points_worst, radius_mean, and concavity_mean. Given that the goal of performing data analytics on this dataset is literally to predict whether a tumor is benign or malignant, a 93% prediction rate is simply astounding. That level of model accuracy would quite literally save lives if applied in the correct ways and if developed with time. This leads us to believe that the dataset's variables are not correlated linearly with diagnosis, but, rather, logistically.
```{r}
df$diagnosis[df$diagnosis == "M"] <- 1
df$diagnosis[df$diagnosis == "B"] <- 0
df$diagnosis <- as.numeric(df$diagnosis)

glm.fit <- glm(diagnosis~radius_mean+concavity_mean+concave_points_worst,data=df,family=binomial)
summary(glm.fit)
#str(df)
glm.probs = predict(glm.fit,type="response")
glm.probs[1:5]

glm.pred=ifelse(glm.probs>0.5,1,0) #if it's >0.5, we call it malignant, otherwise benign
attach(df) #allows us to use variables by name
table(glm.pred,diagnosis) #allows us to compare our predictions to the actual data
mean(glm.pred==diagnosis) #classification performance: gives us proportion
g=glm(diagnosis~radius_mean,data=df,family=binomial)


renderPlot({
  plot(radius_mean,diagnosis,xlab="radius_mean",ylab="diagnosis")
  curve(predict(g,data.frame(radius_mean=x),type="resp"),add=TRUE)
  points(radius_mean,fitted(g),pch=20)
  })
```

**Using ROC and QDA**
Although I played around with many predictors for my different tests (QDA, ROC Curve, and Validation sets), I found area_mean to be consistently capable in terms of predicting false positives and negatives both from the confusion matrix and the ROC curve plot. This could be potentially valuable to both patients and doctors when simply self-analyzing tumors and determining their malignancy since area is something we all have an inherent understanding of.

## **Multi-predictor Linear Regression**
```{r}
fit=lm(radius_mean~concave_points_mean+concavity_mean,data=df)
summary(fit)
par(mfrow=c(2,2))
renderPlot(plot(fit))
```
Because our response variable is categorical (whether the tumor is malignant or benign), we decided to make radius_mean as the response variable for linear regression because it's a quantitative variable. We think using radius_mean as the response variable is fine in this case because malignant tumors typically grow much quicker compared to benign tumors. After plotting several scatterplots, concavity_mean and concave_points_mean appear to be the best predictors for radius_mean.

Looking at the residual plot, the points are randomly scattered, so we can assume that the model is approximately linear with a constant variance. Additionally, the Normal Q-Q plot is close to being linear which means that the model is approximately normal. We have a R ^2 value of 0.7201, which is fairly decent. We can conclude that concavity_mean and concave_points_mean are good predictors for radius_mean which in turn should hopefully mean that they are good predictors for diagnosis.

## **Residual Analysis**
To conduct a regression analysis, we first constructed a linear model of the data, using concave_points_worst to predict diagnosis. Then we created a predicted output variable, and then we compared the prediction to the actual diagnosis value. As the plot clearly illustrates, there is a very apparent correlation between concave_points_worst's increasing values and the residuals produced from the regression analysis. We were initially expecting a scatterplot where the data appeared to be randomly "scattered", but it makes sense that the residuals would be linear since this is a binary prediction - only one of two values theoretically can be obtained. This leads to graphs such as this one that aren't particularly intriguing. The mean R-squared value was only about .63. While this is quite better than a 50/50 shot, it's not nearly as meaningful as the logistic model's results. Thus, regression analysis likely isn't the best method to analyze this dataset because it likely isn't linear.
```{r}

linearcancermodel = lm(diagnosis~concave_points_worst, data = df)

summary(linearcancermodel)
df$DIAGNOSIS_Predicted  = predict(linearcancermodel)
df$Residual = df$diagnosis - df$DIAGNOSIS_Predicted

renderPlot(plot(df$DIAGNOSIS_Predicted,df$Residual,pch=21,bg="red",col="red")+abline(0,0))
```
As can be seen in our second redsidual analysis, the shape/pattern of the graph is nearly identical and doesn't lead to any particularly interesting insights.
```{r}
breastcancer.lm = lm(diagnosis ~ radius_mean, data=df) 
breastcancer.res = resid(breastcancer.lm)
renderPlot(plot(df$radius_mean, breastcancer.res,ylab="Residuals", xlab="radius_mean", main="Breast Cancer Status (Benign vs. Malignant)")+abline(0, 0)) 
```

## **Logistic Regression**
For logistic regression, we constructed a glm.fit using a binomial family. For this dataset, logistic regression worked remarkably well. Despite running many different logistic regressions, comparing the prediction rates of several combinations of variables, the very first logistic regression performed yielded the best results. We constructed a glm.fit using concave_points_worst, radius_mean, and concavity_mean to predict diagnosis. This first analysis led to above a 93% prediction rate (very low false negatives and false positives). Out of all the datasets we've explored throughout this class, no model on any dataset has every yielded results this promising. This leads us to believe that the data within this model is both very valuable and very close to a true logistic function.
```{r}
df$diagnosis[df$diagnosis == "M"] <- 1
df$diagnosis[df$diagnosis == "B"] <- 0
df$diagnosis <- as.numeric(df$diagnosis)

glm.fit <- glm(diagnosis~radius_mean+concavity_mean+concave_points_worst,data=df,family=binomial)
summary(glm.fit)

glm.probs = predict(glm.fit,type="response")
glm.probs[1:5]

glm.pred=ifelse(glm.probs>0.5,1,0) #if it's >0.5, we call it malignant, otherwise benign
attach(df) #allows us to use variables by name
table(glm.pred,diagnosis) #allows us to compare our predictions to the actual data
mean(glm.pred==diagnosis) #classification performance: gives us proportion
g=glm(diagnosis~radius_mean,data=df,family=binomial)

renderPlot({
  plot(radius_mean,diagnosis,xlab="radius_mean",ylab="diagnosis")
  curve(predict(g,data.frame(radius_mean=x),type="resp"),add=TRUE)
  points(radius_mean,fitted(g),pch=20)
  })
```
As can be seen in the logistic model plot, it is clear that the logistic model curve is quite close to the actual data point. This was of course to be expected given the 93% prediction rate, but this plot allows that to be seen visually.



## **Linear Discriminant Analysis**
```{r}
diagnosis1=lda(diagnosis~radius_mean+perimeter_mean+area_mean+compactness_mean+concavity_mean,data=breastdf1,prior=c(1,1)/2,subset=train1)
lda.pred=predict(diagnosis1, breastdf1[-train1, ])
df1 = data.frame(lda.pred)
sliderInput("bins", "Number of bins:", min = 10, max = 50, value = 30)

renderPlot({
  x <- df1$LD1
  bins <- seq(min(x), max(x), length.out = input$bins + 1)
  ggplot(df1) + geom_histogram(mapping = aes(x=LD1), breaks = bins, color="blue") + facet_wrap(~class)})

renderPlot(ggplot(df1) + geom_boxplot(mapping = aes(x=class, y=LD1)))

table(lda.pred$class,breastdf1[-train1, ]$diagnosis)
mean(lda.pred$class==breastdf1[-train1, ]$diagnosis)
```
After running each of the "mean" variables separately in LDA, we picked the best variables to run the analysis. The misclassification rate is low and there's no overlap in LD1. 

## **Quadratic Discriminant Analysis**
```{r}
qda.fit = qda(diagnosis~texture_mean+perimeter_mean+area_mean+radius_mean,data=df,family=binomial)
qda.fit
df.700 = subset(df,area_mean>700)
qda.class = predict(qda.fit, df.700)
table(qda.class$class,df.700$diagnosis)
mean(qda.class$class==df.700$diagnosis)
```

Upon running QDA on several mean measurements of tumors, we found that the best predictor was the mean area of a tumor. In the interval of tumors greater than 700 square units, we predict tumors correctly 95.32% of times. This will be a good predictor for our later insights.  

## **K-Nearest Neighbors**
With K-Nearest Neighbors analysis or KNN, we can use multiple predictors to see which one is best at predicting the results of having a cancerous tumor or not. We set and training data based on the id numbers to use in our analysis. The first KNN analysis done we used perimeter_mean and area_mean and got a results of mean of 83% which is incredibly accurate.
```{r}
# Establishing training data
train = id > 900000

test = cbind(df$perimeter_mean,df$area_mean)

knn.pred = knn(test[train,], test[!train,], df$diagnosis[train], k=1)
table(knn.pred,df$diagnosis[!train])
mean(knn.pred==df$diagnosis[!train])
```
Since these two parameters gave great results, we can use variations of them to see if we can get better results. For this analysis we used perimeter_mean again but used area_se instead. We got much better results with a mean of 84%. 
```{r}
# Establishing training data
train = id > 900000

test = cbind(df$perimeter_mean,df$area_se)

knn.pred = knn(test[train,], test[!train,], df$diagnosis[train], k=1)
table(knn.pred,df$diagnosis[!train])
mean(knn.pred==df$diagnosis[!train])
```
Now that we have improved our results, we can see if we can continue to use the parameters but change our K values, we could potentially get even better results. The analysis consistently determines that a k value of 3 will give the highest mean which is 86%.
```{r}
# Establishing training data
train = id > 900000

test = cbind(df$perimeter_mean,df$area_se)

mean = 1:20
max = 0

for(k in 1:20){
  knn.pred = knn(test[train,], test[!train,], df$diagnosis[train], k)
  mean[k] = mean(knn.pred==df$diagnosis[!train])
  if(mean[k] > max){
     max = mean[k]
     maxk = k
  }
}

renderPlot(plot(mean))

max
maxk

```

## **ROC Curve**
The ROC curve stands for Receiver Operating Characteristic. It provides us with a visual of our true positive rate against our false positive rate as a diagnostic for interpreting our predictors' accuracy. When the curve is a steeper slope, we have higher sensitivity (which is our goal). When the curve is less steep, we have a less accurate test

Below is the ROC curve for smoothness mean:
```{r}

roc(diagnosis ~ smoothness_mean,df)
roc(diagnosis ~ smoothness_mean, df, smooth=TRUE)
renderPlot(plot.roc(df$diagnosis, df$smoothness_mean, percent = TRUE,
            partial.auc=c(100,90), partial.auc.correct= TRUE,
            partial.auc.focus="sens",
            ci=TRUE, boot.n=100, ci.alpha=0.9, stratified=FALSE,
            auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE))
roc(df$diagnosis, df$smoothness_mean)
```
We see that the slope is rather flat (close to 45 degrees). Our test accuracy using smoothness_mean will be only fair. With an area under the curve of 0.7235 (perfect is 1 and failing is 0.5), our predictor is just average.
```{r}
renderPlot(plot.roc(df$diagnosis, df$compactness_mean, percent = TRUE,
            partial.auc=c(100,90), partial.auc.correct= TRUE,
            partial.auc.focus="sens",
            ci=TRUE, boot.n=100, ci.alpha=0.9, stratified=FALSE,
            auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE))
roc(df$diagnosis,df$compactness_mean)
```
Visually, we can see that compactness_mean has a steeper slope. Our value for area under the curve is 0.8638 proving compactness_mean to be a better predictor than smoothness_mean.
```{r}
renderPlot(plot.roc(df$diagnosis, df$area_mean, percent = TRUE,
            partial.auc=c(100,90), partial.auc.correct= TRUE,
            partial.auc.focus="sens",
            ci=TRUE, boot.n=100, ci.alpha=0.9, stratified=FALSE,
            auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE))
roc(df$diagnosis,df$area_mean)
```
Since QDA showed area_mean to be a solid predictor, I hypothesized that the slope of area_mean would be steeper than that of smoothness and compactness. Just as I suspected, the area under the curve for area_mean came out to 0.9383, proving the area_mean predictor to have very high selectivity.

## **Validation**
Validation consists of breaking our data up into separate segments. Using Leave-one-out cross validation, we found the following plot:
```{r}
df$diagnosis[df$diagnosis == "M"] <- 1
df$diagnosis[df$diagnosis == "B"] <- 0
df$diagnosis <- as.numeric(df$diagnosis)

renderPlot(plot(df$diagnosis~df$texture_mean,data=df))

## LOOCV
glm.fit=glm(df$diagnosis~df$texture_mean, data=df)

loocv=function(fit){
  h=lm.influence(fit)$h
  mean((residuals(fit)/(1-h))^2)
}

loocv(glm.fit)
```
Using LOOCV, I was able to parse through the texture_mean data and see its correlation in comparison with the diagnosis of malignant vs. benign. LOOCV takes a single observation from the validation data and uses the other observations in the training data. As the model is created, each observation within our sample is used in the validation data.

## **Cross-Validation**
Choosing a sample training set for which to train upon requires that our sample be representative of the data as a whole. As a result, the validation process of randomly splitting the data in half can yield highly variable test error. We are going to perform K-fold Cross validation for K=5 and K=10 to estimate our test error and then see how they stack up both to each other and to LOOCV which I ran previously.
```{r}
df$diagnosis[df$diagnosis == "M"] <- 1
df$diagnosis[df$diagnosis == "B"] <- 0
df$diagnosis <- as.numeric(df$diagnosis)
## 5-fold CV error
cv.error=rep(0,5)
degree=1:5
for(d in degree){
  glm.fit=glm(diagnosis~poly(texture_mean,d), data=df)
  cv.error[d]=loocv(glm.fit)
}


## 10-fold CV error
cv.error10=rep(0,5)
for(d in degree){
  glm.fit=glm(diagnosis~poly(texture_mean,d), data=df)
  cv.error10[d]=cv.glm(df,glm.fit,K=10)$delta[1]
}
renderPlot(plot(degree,cv.error,type="b")+lines(degree,cv.error10,type="b",col="red"))
#renderPlot(plot(degree,cv.error10,type="b",col="red"))
```
The black line represents our 5-fold cross-validation process and the red line represents our 10-fold cross-validation. Looking at the same 5 subdivisions for each cross-validation, we see a reduction in error simply from increasing the amount of subdivisions, K, by 5.

One short-coming of using K-fold Cross-validation is that the prediction error will be biased upward due to high bias-variance tradeoff. If we choose, LOOCV, we minimize bias but greatly increase variance since K=n. If we choose something like K=5 of 10, our tradeoff is a little better.

## **The Bootstrap**
We choose to create a bootstrap method in addition to cross-validation because bootstrapping allows us to quantify uncertainty for a certain statistical learning method. We have written a function for calculating alpha of the predictor and response variables (texture_mean and tumor diagnosis respectively).
```{r}
alpha=function(x,y){
  vx=var(x)
  vy=var(y)
  cxy=cov(x,y)
  (vy-cxy)/(vx+vy-2*cxy)
}
alpha(df$texture_mean,df$diagnosis)

## What is the standard error of alpha?

alpha.fn=function(data, index){
  with(data[index,],alpha(texture_mean,diagnosis))
}

alpha.fn(df,1:100)

set.seed(1)
alpha.fn (df,sample(1:100,100,replace=TRUE))

boot.out=boot(df,alpha.fn,R=1000)
boot.out
renderPlot(plot(boot.out))
```
Our histogram shows estimates of the alpha values we obtained by generating 1,000 simulated data sets based on the true population. Our mean centers around -0.03704653 which is our true value for alpha. Alpha is a indication of how well our risk has been minimized.

## **Subset Selection**
**Best Subset Selection**
Using best subset selesction we can detrmine which model has the best predictors with the lowest error. With a dataset as large and complex as the one we are using, this helps us narrow down some key predictors.
```{r}

bssdf$diagnosis[bssdf$diagnosis == "M"] <- 1
bssdf$diagnosis[bssdf$diagnosis == "B"] <- 0

#RUnning best subset regression and saving results
regfit.full=suppressWarnings(regsubsets(bssdf$diagnosis~.,data=bssdf, nv=32))
reg.summary=summary(regfit.full)

#Plotting results
min = which.min(reg.summary$cp)
renderPlot(plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp")
           + points(min,reg.summary$cp[min],pch=20,col="red"))
```
From the analysis above we can see that that the model with the lowest error has 14 different predictors. Those predictors can be seen below in the summary of the coefficients of the best subset analysis for this model.
```{r}
#Coefficient Summary
coef(regfit.full,min)
```

**Forward Stepwise**
Now we that we have a genral idea of what kind of results subset selection can reproduce, we can try the analysis again this time using forward stepwise. As one can see from the results below, this analysis yields a model with 16 parameters as the best.
```{r}
bssdf$diagnosis[bssdf$diagnosis == "M"] <- 1
bssdf$diagnosis[bssdf$diagnosis == "B"] <- 0

#RUnning best subset regression and saving results
regfit.forward=suppressWarnings(regsubsets(bssdf$diagnosis~.,data=bssdf, nv=32, method = "forward"))
reg.summary1=summary(regfit.forward)

#Plotting results
min1 = which.min(reg.summary1$cp)
renderPlot(plot(reg.summary1$cp,xlab="Number of Variables",ylab="Cp")
           + points(min1,reg.summary1$cp[min1],pch=20,col="red"))
```
Some of the parameters are also found in the results from the best subset selection but we also have new parameters taken into account such as area_se and compactness_se.
```{r}
#Coefficient Summary
coef(regfit.forward,min)
```

**Backward Stepwise**
Lastly we can repeat the analysis using backward stepwise. Interestingly enough, we actually get the same results as the original analysis, indluding the parameters that make up the best model as seen below.
```{r}

bssdf$diagnosis[bssdf$diagnosis == "M"] <- 1
bssdf$diagnosis[bssdf$diagnosis == "B"] <- 0

#RUnning best subset regression and saving results
regfit.backward=suppressWarnings(regsubsets(bssdf$diagnosis~.,data=bssdf, nv=32, method = "backward"))
reg.summary2=summary(regfit.backward)

#Plotting results
min2 = which.min(reg.summary2$cp)
renderPlot(plot(reg.summary2$cp,xlab="Number of Variables",ylab="Cp")
           + points(min,reg.summary2$cp[min2],pch=20,col="red"))

#Coefficient Summary
coef(regfit.backward,min2)
```

## **Shrinkage Methods**
**Ridge Regression**
Continuing our analysis, we can use shrinkage methods to see which model would give us the best results as well. First we must make a matrix our data and a variable y with the response variable.
```{r}
smdf$diagnosis[smdf$diagnosis == "M"] <- 1
smdf$diagnosis[smdf$diagnosis == "B"] <- 0

x=model.matrix(smdf$diagnosis~.-1,data=smdf) 
y=smdf$diagnosis
y = as.numeric(y)
```
Now we can use that matrix and the response variable to run a ridge regression model using glmnet. THis allows us to visually see the coefficients of the parameters that most affect the model and then it shrinks those coefficients to almost zero.
```{r}
fit.ridge=glmnet(x,y,alpha=0)
renderPlot(plot(fit.ridge,xvar="lambda",label=TRUE))
```
We are able to cross validate this model to see which one produces best results and according to the curve, the full model tends to be showing pretty good results. When looking at the coefficients, we can see that all parameters are in use except for id.
```{r}
cv.ridge=cv.glmnet(x,y,alpha=0)
renderPlot(plot(cv.ridge))
coef(cv.ridge)
```

**Lasso**
SImilarly, we can use glmnet to run an analysis using the lasso model. This graphs looks similar slightly different than the ridge regression model. It seems like lasso is determine that other coefficients have more weight than others.
```{r}
fit.lasso=glmnet(x,y)
renderPlot(plot(fit.lasso,xvar="lambda",label=TRUE))
```
When running cross validation on the lasso model, we can see that while many parameters are still in use, some were not signifacnt enough and were not included in the model.
```{r}
cv.lasso=cv.glmnet(x,y)
renderPlot(plot(cv.lasso))
coef(cv.lasso)
```

## **Decision Trees**
Decision trees are great for this kind of analysis because he already have a binary response variable. We have to do some set up first in order for the tree to analyz the data correctly.
```{r}
# Setting up the data
df$diagnosis[df$diagnosis == 1] <- "M"
df$diagnosis[df$diagnosis == 0] <- "B"

result = df$diagnosis

df$diagnosis[df$diagnosis == "M"] <- 1
df$diagnosis[df$diagnosis == "B"] <- 0

df$diagnosis = as.numeric(df$diagnosis)

df=data.frame(df, result)
```
Once this set up is compelete, we can do a tree and analyze the kinds of parameters and thresholds it deems significant.
```{r}
tree.diagnosis=tree(result~.-diagnosis,df)
summary(tree.diagnosis)
renderPlot({
  plot(tree.diagnosis)
  text(tree.diagnosis,pretty=0, cex = 0.7)
})
```
After producing the tree above we can see that the big factor seems to be perimeter_worst. Most of the predictions on the right or greater than 105.95 seem to produce a result showing a malignant tumor. Meanwhile, predictions using perimeter_worst less than 1005.95 mostly seem to produce reults of benign tumors.

**Random Forest**
We continue our analysis of using decision trees this time as random forest. Random forrest reproduces many bushy trees and averages the error. The biggest variable we can change with this is the size of the training data.
```{r}
set.seed(101)
train=sample(1:nrow(df),100)

rf.diagnosis=randomForest(result~.-diagnosis,data=df,subset=train)
rf.diagnosis
```
With a training data of 100, we got incredibly accurate results with an error rate of 6%. 
```{r}
set.seed(101)
train=sample(1:nrow(df),200)

rf.diagnosis=randomForest(result~.-diagnosis,data=df,subset=train)
rf.diagnosis
```
Increasing the size of the training data improved the accuracy slightly by repoducing an error rate of 5%
```{r}
set.seed(101)
train=sample(1:nrow(df),400)

rf.diagnosis=randomForest(result~.-diagnosis,data=df,subset=train)
rf.diagnosis
```
Finally, creating a training data of size 400, we get the best results with a 3% error rate, which is increadibly accurate results.This again can be due to the nature of the dataset which seems to be set up very well for this kind of analysis.

**Boosting**
Boosting is another way to analyze decision trees. This method instead creates many small tree. The function allows us to play arround with the amount of trees, the depth and the division.

```{r}
set.seed(101)
train=sample(1:nrow(df),400)

boost.diagnosis=gbm(result~.-diagnosis,data=df[train,],distribution="gaussian",n.trees=1000,shrinkage=0.001,interaction.depth=2)
renderPlot(summary(boost.diagnosis))
```

```{r}
set.seed(101)
train=sample(1:nrow(df),400)

boost.diagnosis=gbm(result~.-diagnosis,data=df[train,],distribution="gaussian",n.trees=1000,shrinkage=1,interaction.depth=3)
renderPlot(summary(boost.diagnosis))
```

```{r}
set.seed(101)
train=sample(1:nrow(df),400)

boost.diagnosis=gbm(result~.-diagnosis,data=df[train,],distribution="gaussian",n.trees=1000,shrinkage=0.1,interaction.depth=4)
renderPlot(summary(boost.diagnosis))
```

We ran three analysis, each with a different shrinkage and depth value but the results remained consistent. The most significant predictors using boosting are concave_points_worst, area_worst, and premieter_worst.

## **Support Vector Machines**

**Nonlinear Support Vector Machine**
The following support vector machine is a nonlinear model with a radial kernel used to predict cancer diagnosis using radius_mean. The mean value calculated is the misclassification rate (11%), which demonstrates that this model is just slightly less impressive than the logistic regression model for this dataset. This result validates our insight that this data is not fit to a linear model, as demonstrated by the fact that both a nonlinear support vector machine and a logistic regression model significantly outperformed the linear regression analysis.
```{r}

renderPlot(plot(radius_mean,col=diagnosis+3))
y<-diagnosis
x<-radius_mean

#convert data to a data frame
data<-data.frame(y=factor(y),x)
head(data)
Radialsvm=svm(factor(y) ~ .,data=data,kernel="radial",cost=5,scale=F)
Radialsvm
#confusion matrix to check the accuracy
table(predicted=Radialsvm$fitted,actual=data$y)
#misclassification Rate
mean(Radialsvm$fitted!=data$y)*100 #11% misclassification rate
```

**Linear Support Vector Machine**
Next we constructed a linear support vector machine with the hypothesis that it would yield unimpressive results, as we are fairly confident that the dataset simply is not linear. It turns out we were correct, as the error was 0.3116. This is slightly better than the strictly linear model, but still underwhelming compared to the nonlinear support vector machine.
```{r}
rad_mean=c(radius_mean)
diag=c(diagnosis)
train=data.frame(rad_mean,diag)
model <- lm(diag ~ rad_mean, train)
model_svm <- svm(diag~rad_mean,train)
pred <- predict(model_svm,train)
renderPlot(plot(train,pch=16)+abline(model)+points(train$rad_mean, pred, col="blue", pch=4))

error <- model$residuals
lm_error <- sqrt(mean(error^2))
lm_error #lm_error
error_2 <- train$diag - pred
svm_error <- sqrt(mean(error_2^2)) 
svm_error #svm_error
```
The black dots represent our data points of radius_mean and the corresponding diagnosis. The black line shows a simple linear regression on the data highlighting a general positive trend. The blue cross marks show us how the model trained on the data and created its own prediction set. We can see these blue marks have a much tighter trend than the black ones. 

## **Unsupervised Learning**

**PCA**
```{r}
breastmatrix = matrix(
  c(radius_mean,texture_mean,perimeter_mean,area_mean,smoothness_mean,compactness_mean,concavity_mean,concave_points_mean,symmetry_mean,fractal_dimension_mean,radius_se,texture_se,perimeter_se,area_se,smoothness_se,compactness_se,concavity_se,concave_points_se,symmetry_se,fractal_dimension_se,radius_worst,texture_worst,perimeter_worst,area_worst,smoothness_worst,compactness_worst,concavity_worst,concave_points_worst,symmetry_worst,fractal_dimension_worst), 569, 30)

dimnames(breastmatrix) = list(
  c("M","M","M","M","B","B","B","B","M", "B", "B", "M", "B", "M", "B", "B", "B", "B", "B", "B", "B", "M", "M", "M", "M", "B","M", "M", "B", "B", "B", "M", "M", "B", "M", "M", "B", "M", "M", "B", "M", "M", "B", "B", "M", "B", "M", "B", "M", "B","B", "B","B", "M", "B", "B", "B", "M", "B", "B", "B", "B", "B", "B", "M", "B", "B", "B", "M", "B", "M", "M", "B", "M", "B", "B", "M", "M","B", "M", "B", "M", "M", "B", "B", "B", "B", "M", "B", "B", "M", "M", "M", "M", "B", "M", "B", "M", "B", "B", "B", "B", "M", "B", "B", "M", "M", "B", "M", "M", "M", "M", "M", "B", "B", "M","M", "M", "B", "M", "B", "M", "B", "B", "B", "M", "B", "M", "M", "M","M", "B", "B", "M", "M", "M", "B", "B", "B", "M", "B", "B", "B", "B", "B", "M", "M", "M", "B", "B", "M", "B", "B", "M", "M", "B","M", "M", "B", "B", "B", "B", "M", "B", "B", "B", "B", "B", "M", "M", "B", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M","M", "M", "M", "M", "B", "B", "B", "B", "M", "B", "B", "M", "B", "M", "B", "B", "M", "B", "B", "M", "M", "B", "M", "M", "B", "B","B","B", "B", "B", "M", "B", "B", "B", "B", "B", "B", "B", "M", "B", "B", "M", "M", "M", "B", "M", "B", "B", "B", "B", "B", "B","B", "M", "B", "B", "B", "B", "B", "B", "B", "M", "B", "B", "M", "B", "M", "B", "M", "B", "B", "B", "B", "M", "M", "M", "M", "B", "B", "B", "B", "M", "B", "M", "B", "M", "M", "B", "B", "B", "M", "B", "B", "B", "B", "B", "B", "M", "B", "M", "M", "M", "B", "B","B", "B", "B", "B", "M", "B", "B", "B", "B", "B", "M", "M", "B", "M", "M", "B", "M", "B", "M", "M", "B", "B", "B", "B", "B", "M","M", "B", "B", "B", "B", "B", "M", "B", "B", "B", "M", "M", "B", "B", "M", "M", "B", "B", "B", "B", "B", "B", "M", "M", "M", "B","B", "B", "B", "B", "B", "B", "M", "B", "M", "B", "B", "B", "B", "M", "B", "B", "M", "B", "B", "M", "B", "B", "B", "B", "B", "B","B", "B", "B", "B", "M", "M", "B", "M", "M", "B", "M", "B", "B", "B", "B", "M", "B", "M", "B", "B", "M", "B", "M", "B", "B", "M","M", "B", "M", "B", "B", "B", "B", "B", "B", "B", "B", "B", "M", "M", "B", "B", "B", "B", "B", "B", "M", "B", "M", "B", "B", "B","B", "B", "B", "B", "B", "B", "M", "B", "B", "B", "B", "B", "B", "B", "B", "M", "B", "M", "B", "B", "B", "M", "B", "B", "B", "B", "B", "M", "M", "M", "B", "B", "M", "B", "M", "B", "B", "B", "B", "B", "M", "B", "B", "B", "M", "B", "M", "B", "M", "M", "B", "B","B", "B", "M", "B", "B", "B", "B", "B", "B", "B", "B", "M","B", "B", "B", "M", "B", "M", "M", "B", "B", "B", "M", "B", "B", "B","B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "B", "M", "B", "B", "M", "M", "M", "M", "M","M", "B", "M", "B", "B", "M", "B", "B", "M", "B", "M", "M", "B", "B", "B", "B", "M", "M", "B", "M", "M", "B", "M", "B", "M", "M","B", "M", "B", "B", "M", "M", "B", "M", "M", "M", "B", "B", "M", "B", "M", "B", "B", "M", "M", "B","B", "B", "M"),
  c("radius_mean","texture_mean","perimeter_mean","area_mean","smoothness_mean","compactness_mean","concavity_mean","concave_points_mean","symmetry_mean","fractal_dimension_mean","radius_se","texture_se","perimeter_se","area_se","smoothness_se","compactness_se","concavity_se","concave_points_se","symmetry_se","fractal_dimension_se","radius_worst","texture_worst","perimeter_worst","area_worst","smoothness_worst","compactness_worst","concavity_worst","concave_points_worst","symmetry_worst","fractal_dimension_worst")
)

apply(breastmatrix,2,mean)
apply(breastmatrix,2, var)
pca.out=prcomp(breastmatrix, scale=TRUE)
renderPlot(biplot(pca.out, scale=0))
```

**K-Means Clustering**
```{r}
breastdf <- data.world::query(
  data.world::qry_sql("SELECT * FROM data"),
  dataset = project
)
renderPlot({
  ggplot(breastdf,(aes(x=radius_worst, y=concavity_worst, colour = as.factor(diagnosis)))) + geom_point()
})

set.seed(570)
km.out=kmeans(breastmatrix,2,nstart=15)
breastdf3 <- data.frame(breastdf, km.out$cluster)
breastdf4 <- data.frame(km.out$centers, km.out$size)
renderPlot({ggplot(breastdf3) + geom_point(mapping = aes(x=radius_worst, y=concavity_worst, colour = as.factor(km.out.cluster))) + geom_point(data=breastdf4, mapping=aes(radius_worst, concavity_worst, size=km.out.size))})
```
While there is some overlap, radius_worst and concavity_worst gives us clusters that are distinct which is what we expected because these variables were part of the best subset regression. Additionally, the "worst" variables most likely play into the distinction of malignant and benign observations because the values are extreme. 

**Hierachical Clustering**
```{r}
hc.complete=hclust(dist(breastmatrix),method="complete")
renderPlot(plot(hc.complete))
hc.cut=cutree(hc.complete,2)
table(hc.cut,diagnosis)
table(hc.cut,km.out$cluster)
```
The hierarchical dendrogram wasn't helpful because there are so many rows. But looking at the tables above, 81 of the malignant observations are included in cluster 2 which is dominated by benign observations. There seems to be a bigger overlap than we originally thought. 