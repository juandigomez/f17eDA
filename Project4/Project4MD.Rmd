---
title: "Project 4: Prediction of 2016 Election"
author: "Group 8: Jackie Lee, Juan Gomez, Patrick Lyons, Ross Miglin"
resource_files:
- .Renviron
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 6
    toc_float: yes
runtime: shiny
---

```{r setup, include=FALSE}
require(data.world)
require(MASS)
require(ISLR)
require(dplyr)
require(ggplot2)
require(hexbin)
require(jsonlite)
require(shiny)
require(rsconnect)
require(knitr)
require(lubridate)
require(rmarkdown)
require(tidyr)
require(glmnet)
library(ggplot2)
library(class)
library(glmnet)
library(leaps)
library("e1071")
require(tree)
require(randomForest)
require(gbm)
knitr::opts_chunk$set(echo = TRUE)
```

## **R Session Info**
```{r}
sessionInfo()
```

## **Introduction**
In order to analyze the methods discussed in class for model comparison, we decided to use data on the 2016 Presidential Election in order to make predictions of which candidate one based on different races and different occupations. This is especially interesting because we now know what factors where key in the success Trump. So we can make models and assumptions based on that information to see if these factors are as significant in our models as the were in real life.


## **Requiring data and initializing session**
We have retrieved our data and required the proper libraries in order to correctly compute our analysis. Our data can be viewed at https://data.world/juandi/f-17-eda-project-4.
```{r}
require(data.world)

project <- "https://data.world/juandi/f-17-eda-project-4"
data.world::set_config(cfg_env("DW_API"))
electiondf <- data.world::query(
  data.world::qry_sql("SELECT * FROM electiondata"),
  dataset = project
)

attach(electiondf)
```

## **Decision Trees**
Decision trees are forms of making tree based prediction. For us, will use decision trees to determine what factors are best for predicting the candidate in the 2016 election data.Before using the decision trees, we must first transform the data to get an accurate output.

```{r}
# Setting up the data
df = electiondf
result = df$candidate

df$candidate[df$candidate == "Trump"] <- 1
df$candidate[df$candidate == "Clinton"] <- 0

drops <- c("state")
df <- df[ , !(names(df) %in% drops)]

df$candidate = as.numeric(df$candidate)
df$candidate = as.numeric(df$votes)

df=data.frame(df, result)
```
Once the data has been transform, we are able to use the tree library in order to produce a decision tree. As one can see, using this model graduate degrees and asian_american_pop were the biggest determine of the winner. Also notice that regardless of the asian_american_pop, if you graduate_degrees were below a certain threshhold, the model predicted Trump. This reflects that actual results of the election as it was the more educated population that voted for Clinton and people with either high school diplomas or no formal education who voted for Trump.

```{r}
tree.candidate=tree(result~.,df)
summary(tree.candidate)
renderPlot(plot(tree.candidate))
```

**Random Forest**
Using similar techniques, we can create random forest models to predict the data as well. These models create many bushy trees and averages them to reduce the variance. Using the randomforest library, we can create these models and interpret the results. To create these models, we must create a set of training data first. Consdiering we have 50 rows, we have set our training data to 20 items.

```{r}
set.seed(101)
train=sample(1:nrow(df),20)
rf.election=randomForest(result~.-candidate,data=df,subset=train)
rf.election
```
The results using this training data was about 20% as seen above. The model had more misclassification when classifying Clinton than Trump. Overall not terrible results, but could definitely be imporved.

```{r}
set.seed(101)
train1=sample(1:nrow(df),30)

rf.election1=randomForest(result~.-candidate,data=df,subset=train1)
rf.election1
```
To try to improve the model, we decided to increase the size of the training data from 20 to 30. As one can see in the results above, this did improve the accuracy of the model by about 7%.

```{r}
set.seed(101)
train2=sample(1:nrow(df),45)

rf.election2=randomForest(result~.-candidate,data=df,subset=train2)
rf.election2
```
The more data we can use in our training data, the more accurate our model will be. We increased the training data size again from 30 to 45 and got an estimated error of 8% which is a much better result than the training data we originally used.

**Boosting**
Boosting is another tree based technique that creates many small trees in order to conduct its analysis. For our project, we changed the different parametes one can include in the boosting function to see what happens. We will use the training data that gave us the best results.

```{r}
set.seed(101)
train=sample(1:nrow(df),45)

boost.election=gbm(result~.,data=df[train,],distribution="gaussian",n.trees=1000,shrinkage=0.001,interaction.depth=2)
renderPlot(summary(boost.election))
```
For this first model, we saw graduate_degree and asian_american_pop be the two most influential predictors. However, we will continue to change the amount of trees, the shrinkage and the interaction depth to see if our results change.

```{r}
boost.election=gbm(result~.,data=df[train,],distribution="gaussian",n.trees=100,shrinkage=1,interaction.depth=3)
renderPlot(summary(boost.election))
```
We decreased the amount of trees and increased the shortage and depth of the the model. This time graduate_degree still was the most influential predictor however black_pop was the second most closely followed by white_pop. This refelct the actual election data a little more closely as race was a major factor in terms of how people voted. Most of the white vote ended up going to Trump and most of the minority vote went to Clinton.

```{r}
boost.election=gbm(result~.,data=df[train,],distribution="gaussian",n.trees=10000,shrinkage=0.01,interaction.depth=4)
renderPlot(summary(boost.election))
```
Finally we increased the amount of trees and the depth but decreased the shrinkage. This ended up giving similar results to the first boosting model with graduate_degree and asian_american_pop as the top two most influential predictors.

```{r}
renderPlot(plot(boost.election,i="graduates_degree"))
renderPlot(plot(boost.election,i="asian_american_pop"))
```
We can now plot these predictors to see exactly what was the result with these predictors. As the percent graduate degrees increase, most people voted for Clinton (represented by 0). Same with the asian american population. As percentage increased, the Clinton was predicted. So because of the dramatic behavior of these two predictors we can see why they were significant in the models.

## **Support Vector Machines**
Support vector machines are a form of fitting a prediction model to the data provided using tuning parameters and methods of cutting up the data in higher order planes. Each plane that SVM technique uses to cut up data divides the data into segments for which there are distinct charcteristics.
```{r}
grad_deg=c(graduates_degree)
percent_dem=c(pct_dem)
train=data.frame(grad_deg,percent_dem)
plot(train,pch=16)
model <- lm(percent_dem ~ grad_deg, train)
abline(model)
model_svm <- svm(percent_dem~grad_deg,train)
pred <- predict(model_svm,train)
points(train$grad_deg, pred, col="blue", pch=4)
```
The black dots represent our data points of percentages of individuals with graduate degrees in each state and the corresponding percentage of democrats in those same states. The black line shows a simple linear regression on the data highlighting a general positive trend. The blue cross marks show us how the model trained on the data and created its own prediction set. We can see these blue marks have a much tighter trend than the black ones.
```{r}
error <- model$residuals
lm_error <- sqrt(mean(error^2)) # 3.832974
error_2 <- train$percent_dem - pred
svm_error <- sqrt(mean(error_2^2)) # 2.696281
svm_tune <- tune(svm, percent_dem ~ grad_deg, data = train,
                 ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9))
)
print(svm_tune)
best_mod <- svm_tune$best.model
best_mod_pred <- predict(best_mod, train) 

error_best_mod <- train$percent_dem - best_mod_pred 
best_mod_RMSE <- sqrt(mean(error_best_mod^2)) # 1.290738 

plot(svm_tune)

plot(train,pch=16)
points(train$grad_deg, best_mod_pred, col = "blue", pch=4)
```
## **Unsupervised Learning**
Lastly, we will try to further our knowledge of our data through unsupervised learning and to find interesting things. In this section, we're going to analyze our data through principal components analysis and clustering.

**Principal Components Analysis**
```{r}
electmatrix = matrix(
  c(white_pop,latino_pop,high_school,bachelors), 50, 4)

dimnames(electmatrix) = list(
  c("Alabama","Arizona","Hawaii","Idaho","Illinois","Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Arkansas","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","California","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania","Rhode Island","South Carolina","Colorado","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming","Connecticut","Delaware","District of Columbia","Florida","Georgia"),
  c("white_pop","latino_pop","high_school","bachelors")
)

apply(electmatrix,2,mean)
apply(electmatrix,2, var)
pca.out=prcomp(electmatrix, scale=TRUE)
pca.out
names(pca.out)
renderPlot(biplot(pca.out, scale=0))
```
After running all of the variables, the four variables with the highest variance are: white_pop, latino_pop, high_school, and bachelors. This visual describes the clusters of states by population of white, population of latinos, high_school, and bachelors. We can observe that the arrows for bachelors and high_school are generally pointing towards states in the Northeast where the level of education is considered to be higher compared to other parts of the nation.

**K-Means Clustering**
```{r}
electmatrix2 = matrix(
  c(bachelors,pct_dem), 50, 2)

dimnames(electmatrix2) = list(
  c("Alabama","Arizona","Hawaii","Idaho","Illinois","Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Arkansas","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","California","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania","Rhode Island","South Carolina","Colorado","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming","Connecticut","Delaware","District of Columbia","Florida","Georgia"),
  c("bachelors","pct_dem")
)

renderPlot({
  ggplot(electiondf,(aes(x=pct_dem, y=bachelors, colour = as.factor(candidate)))) + geom_point()
  })

set.seed(51)
km.out=kmeans(electmatrix2,3,nstart=15)
km.out
electiondf <- data.frame(electiondf, km.out$cluster)
electiondf1 <- data.frame(km.out$centers, km.out$size)
renderPlot({ggplot(electiondf) + geom_point(mapping = aes(x=pct_dem, y=bachelors, colour = as.factor(km.out.cluster))) + geom_point(data=electiondf1, mapping=aes(pct_dem, bachelors, size=km.out.size))})
```
Looking at the basic ggpoint plot, we can see that generally, the states that voted for Clinton are higher in both pct_dem and bachelors which is obvious and expected respectively. Since we know higher in pct_dem and bachelors will most likely be a blue state and the opposite being likely for a red state, we want to analyze the cluster in the middle. Interestingly, some of the states in the middle cluster (Minnesota, Wisconsin, Virginia, and Pennsylvania) are swing states. Overall, the results are as expected and confirms that the predictors chosen by the LDA analysis are good. States with higher level of education (bachelor's) mostly voted for Clinton and some of those that voted for Trump are considered swing states.

**Hierarchical Clustering**
```{r}
hc.complete=hclust(dist(electmatrix2),method="complete")
renderPlot(plot(hc.complete))
hc.single=hclust(dist(electmatrix2),method="single")
renderPlot(plot(hc.single))
hc.average=hclust(dist(electmatrix2),method="average")
renderPlot(plot(hc.average))

hc.cut=cutree(hc.complete,2)
table(hc.cut,candidate)
hc.cut
table(hc.cut,km.out$cluster)
```
Using the same two predictors from K-Means clustering, bachelors and pct_dem, we will now look at hierarchical clustering. Hierarchical clustering dendrogram shows which states are close to each other in terms of bachelors and pct_dem. We can see that a lot of states in the Northeast are clumped together to no surprise. But surprisingly, California and New York are on the other side of the dendrogram. This could be due to a small spread between the observations.

**Comparison to findings in Project 3**
From Project 3, we saw that the LDA model found graduates_degree, asian_american_pop, bachelors, and pct_dem to be strong predictors. From the K-Means clustering, the clusters confirm the findings in the LDA model. States clustered together with higher level of education (bachelor's) mostly voted for Clinton and some of those that voted for Trump are considered swing states.

## **Interesting Findings**
Our insights can be found here: https://data.world/juandi/f-17-eda-project-4/insights

**Decision Trees Interesting Findings**
The effectiveness of decision trees were a little bit tougher to compare using this data set because of how few rows it contained. However, in order to compare the results to our previous project, the same data set had to be used. Additionally, the results are consistent throughout the the decision tree analysis and reflects some of the results found in the previous project.

The decision tree analysis including random forest and boosting consitently place graduate_degree and asian_american_pop as major influencers in the accuracy of the model. While these were all of the major predictors found in project 3, they were definetly some of the more significant predictors that some of the models found in project 3 that popped up in the K nearest neighbors analysis and the LDA analysis.

**Support Vector Machines Interesting Findings**
Using support vector machines for Project 4 really showed how the trends in the predictors can be subtle and not easily caught by our past models. In project 3, we used discriminant analysis to find trends in the data. With QDA, looking at different occupations from state-to-state, I plotted a trend using discrminant analysis in how well occupation worked as a predictor. Although I wasn't able to use the same variables for non-linear SVM as I'd used in Project 3, seeing how tight of a trend SVM produced spoke for itself.

Since our model iteratively calculated a difference between the prediction and actual data, tightness of the resulting blue predicted coordinate points allow for us to showcase a more accurate depiction of what the data is telling us. The 10-fold validation step after we ran our initial training and fit provides the analyst the ability to tune the parameters behind the svm as tightly or as loosely as they want. We learned in class that having the training data too close to the test data can cause for unwanted bias and less accuracy in our model. But the tuning parameter of SVM allows us to tweak the model as much or as little as we need to.  

**Unsupervised Learning Interesting Findings**
In the beginning, we played around with the data to find interesting things which is what unsupervised learning is about. One interesting thing we found was through K-Means clustering. A lot of statistics stated that black voter turnout fell drastically in 2016 from 66.6% in 2012 to 59.6% in 2016. This graph confirms that drastic decrease. We can see that five states considered as mid to high in black population went red and those states could have been blue in 2012.

In order to compare our results to the findings in Project 3 (specifically from LDA Analysis), we ran predictors that were used in the last project and played around using both clustering and pca. One interesting finding was from the hierarchical clustering. We ran bachelors and pct_dem as our variables and we saw that a lot of states in the Northeast were clumped together to no surprised. But surprisingly, California and New York were on the other side of the dendrogram. This could be due to a small spread between the observations.

